model:
  type: decoder
  vocab_size: 32000
  emb_dim: 1024                  
  num_layers: 28
  num_heads: 32                   
  num_query_groups: 8
  max_seq_len: 2048               
  use_rotary_embeddings: true     
  norm_eps: 1.0e-5                
  ffn_hidden_size: 4096
  dropout: 0.1
  initializer_range: 0.02

training:
  optimizer: adamw
  learning_rate: 3.0e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  warmup_steps: 2000
  lr_scheduler: cosine
  gradient_clipping: 1.0
  batch_size: 512
  micro_batch_size: 16
  max_steps: 200000
  eval_interval: 1000
  save_interval: 5000
  seed: 42
  mixed_precision: bf16